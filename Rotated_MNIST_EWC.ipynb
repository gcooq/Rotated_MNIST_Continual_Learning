{"cells":[{"metadata":{"id":"5PMv4Ujouvig","outputId":"f4bdac49-1590-4f66-d28a-62807c7881fc","trusted":true},"cell_type":"code","source":"!free -m\n!df -h\n!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"id":"Y-9QfxcdvepD","outputId":"7faacffb-d821-4a6f-fafa-b37c6e9cb12f","trusted":true},"cell_type":"code","source":"# !pip install --upgrade torch torchvision\nimport torch\nprint('Torch', torch.__version__, 'CUDA', torch.version.cuda)","execution_count":null,"outputs":[]},{"metadata":{"id":"wBo0-zEvwdQo","outputId":"cdb8fc6a-3dda-4db8-da57-b92132a47c56","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import load\nimport random\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"id":"I6NOcqZey8Eo","outputId":"7d7c9a08-8feb-42e4-a5b5-7189df158d80","trusted":true},"cell_type":"code","source":"# Download dataset from Continual AI Colab's repository\n!git clone https://github.com/ContinualAI/colab.git continualai/colab","execution_count":null,"outputs":[]},{"metadata":{"id":"cPA4URJWy_9b","outputId":"98f2dd97-f953-424e-9ffb-9323be2513e4","trusted":true},"cell_type":"code","source":"# Import the script \nfrom continualai.colab.scripts import mnist\nmnist.init()","execution_count":null,"outputs":[]},{"metadata":{"id":"CgJYVPzuzeYm","trusted":true},"cell_type":"code","source":"# Load dataset:\nx_train, t_train, x_test, t_test = mnist.load()\nprint(\"x_train dim and type: \", x_train.shape, x_train.dtype)\nprint(\"t_train dim and type: \", t_train.shape, t_train.dtype)\nprint(\"x_test dim and type: \", x_test.shape, x_test.dtype)\nprint(\"t_test dim and type: \", t_test.shape, t_test.dtype)","execution_count":null,"outputs":[]},{"metadata":{"id":"FjrzUNkJzjSP","trusted":true},"cell_type":"code","source":"# Display 4 images inside dataset\nf, axarr = plt.subplots(2,2)\naxarr[0,0].imshow(x_train[1, 0], cmap=\"gray\")\naxarr[0,1].imshow(x_train[2, 0], cmap=\"gray\")\naxarr[1,0].imshow(x_train[3, 0], cmap=\"gray\")\naxarr[1,1].imshow(x_train[4, 0], cmap=\"gray\")\nnp.vectorize(lambda ax:ax.axis('off'))(axarr);","execution_count":null,"outputs":[]},{"metadata":{"id":"pKyuB3hzZasl","trusted":true},"cell_type":"code","source":"# switch to cuda GPU\n# Q: Why using GPU? \n# A: cuz for some highly parallelizable problems, GPU can ensure us speedup computation \n\nuse_cuda = True\nuse_cuda = use_cuda and torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\");\nprint(device)\n# Use torch.manual_seed() to seed the RNG our CUDA devices\ntorch.manual_seed(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Part I:  instantiate model and start the training on original data"},{"metadata":{"id":"h8QpcX1oYgG2","trusted":true},"cell_type":"code","source":"# After comparing CNN and MLP's performance on MNIST, we decided to use MLP as the basic network architecture for this project\n# Our MLP model consists of four fully connected hidden layers -- with 512, 256,128,10 units in each hidden layer by themselves;\n# And use Rectified Linear Units(ReLUs) as activation function for each hidden layer;\n# We also include a dropout layer (p=0.2) to precent the overfitting of data\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 10)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, input):\n        x = F.relu(self.fc1(input))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.dropout(x, training=self.training)\n        x = F.relu(self.fc4(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"id":"BoFb5HDlYsTz","trusted":true},"cell_type":"code","source":"# We also compared different optimizers' performance including Stochastic Gradient Descent(SGD), \n# SGD with momentum, RMSprop and Adam. And we decided to use Adam since it has the highest training and validation accuracies\n# for each epoch(94%~97%) while SGD has the lowest for each epoch where yields (84% ~ 89%).\n\nmodel = MLP().to(device)\n# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer =  optim.Adadelta(model.parameters())\n\n# Here we also tried out different batch_size ranging from 128 to 512, and finally decided to use batch size as 200 since this gave\n# us the highest accuracy among all others \nbatch_size = 200","execution_count":null,"outputs":[]},{"metadata":{"id":"EM23x20LYk-_","trusted":true},"cell_type":"code","source":"def train(model, device, x_train, t_train, optimizer, epoch):\n    model.train()\n    \n    for start in range(0, len(t_train)-1, batch_size):\n      end = start + batch_size\n      x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n      x, y = x.to(device), y.to(device)\n#     Reshape the input size for the input of MLP:\n      x = x.view(x.size(0), -1)\n#     Set the gradients to zero before starting to do backpropragation:\n      optimizer.zero_grad()\n\n      output = model(x)\n      loss = F.cross_entropy(output, y)\n      loss.backward()\n      optimizer.step()\n    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n\ndef test(model, device, x_test, t_test):\n#   Use model.eval() for batch norm and dropouts\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for start in range(0, len(t_test)-1, batch_size):\n      end = start + batch_size\n      with torch.no_grad():\n        x, y = torch.from_numpy(x_test[start:end]), torch.from_numpy(t_test[start:end]).long()\n        x, y = x.to(device), y.to(device)\n        x = x.view(x.size(0), -1)\n        output = model(x)\n        # sum up batch loss\n        test_loss += F.cross_entropy(output, y).item() \n        # get the index of the max logit\n        pred = output.max(1, keepdim=True)[1] \n        correct += pred.eq(y.view_as(pred)).sum().item()\n\n    test_loss /= len(t_train)\n    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(t_test),\n        100. * correct / len(t_test)))\n    return 100. * correct / len(t_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(1, 3):\n  train(model, device, x_train, t_train, optimizer, epoch)\n  test(model, device, x_test, t_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"JaTHO_sNZ90l"},"cell_type":"markdown","source":"# Part I Summary: \nSince we used the starter code from Continual AI Colab's repository, in the starter code, the network architecture used CNN and the final accuracy is **94%.** By changing architecture and improving parameterization we imporve the accuracy to **97%.** By improving the basic architecture, it will hlep to improve the performance of continual learning task on different model"},{"metadata":{"id":"YypT_o1JDDge","trusted":true},"cell_type":"code","source":"# Load MNIST again for rotated \nx_train_2, t_train_2, x_test_2, t_test_2 = mnist.load()\n\nprint(\"x_train dim and type: \", x_train_2.shape, x_train_2.dtype)\nprint(\"t_train dim and type: \", t_train_2.shape, t_train_2.dtype)\nprint(\"x_test dim and type: \", x_test_2.shape, x_test_2.dtype)\nprint(\"t_test dim and type: \", t_test_2.shape, t_test_2.dtype)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part II : Rotate MNIST for different tasks"},{"metadata":{"id":"Jz_Or5V2Sdn0","trusted":true},"cell_type":"code","source":"# Here we developed our own rotation function other than importing other rotated MNIST dataset thus we only import data once \n# and the data format is consistant\ndef rotate_dataset(d, seed):\n    torch.manual_seed(seed)\n    print(\"starting rotation...\")\n    rand_rotate = torch.rand(1)\n#     print( rand_rotate)\n#   for each rotation of each task, we rotate them for a random angle between 0 to 360\n    rotation = 360*rand_rotate \n#     print(rotation)\n    rotated_mnist = np.ndarray((d.shape),np.float32)\n    result = []\n    new_d=[]\n\n    for i in range(d.shape[0]):\n        img = Image.fromarray(d[i][0])\n#       Use the roattion fuction from PIL's Image Library \n        img = img.rotate(rotation)\n        rotated_mnist[i,0]= img\n\n    return rotated_mnist","execution_count":null,"outputs":[]},{"metadata":{"id":"L4AkxIkpD0ms","trusted":true},"cell_type":"code","source":"# Rotate MNIST dataset \nx_train_2 = rotate_dataset(x_train, 1)\nx_test_2 = rotate_dataset(x_test, 1)\nprint(\"x_train dim and type: \", x_train_2.shape, x_train_2.dtype)\nprint(\"x_test dim and type: \", x_test_2.shape, x_test_2.dtype)\n# Displat rotated MNIST dataset\nf, axarr = plt.subplots(1,2)\naxarr[0].imshow(x_train[18,0], cmap=\"gray\")\naxarr[1].imshow(x_train_2[18,0], cmap=\"gray\")\nnp.vectorize(lambda ax:ax.axis('off'))(axarr);","execution_count":null,"outputs":[]},{"metadata":{"id":"FW2zdBmSaQvS","trusted":true},"cell_type":"code","source":"# Test our model trained by non-rorated on rotated which only has 18% accuracy.\nprint(\"Testing on the first task:\")\ntest(model, device, x_test, t_test)\n\nprint(\"Testing on the second task:\")\ntest(model, device, x_test_2, t_test);","execution_count":null,"outputs":[]},{"metadata":{"id":"wJh77GcDaUtP","trusted":true},"cell_type":"code","source":"# Finetune  our model using the new roated training set can reach a good acc of 97% \nfor epoch in range(1, 3):\n  train(model, device, x_train_2, t_train, optimizer, epoch)\n  test(model, device, x_test_2, t_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"iDNzZ5vkaX2O","trusted":true},"cell_type":"code","source":"# However if we use our finetuned model on the original dataset, the acc is still very low:\nprint(\"Testing on the first task:\")\ntest(model, device, x_test, t_test);\nprint(\"Testing on the second task:\")\ntest(model, device, x_test_2, t_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"iYZDwAPibFYw","trusted":true},"cell_type":"code","source":"# task 1\ntask_1 = [(x_train, t_train), (x_test, t_test)]\n\n# task 2\ntask_2 = [(x_train_2, t_train), (x_test_2, t_test)]\n\n# task 3\nx_train_3 = rotate_dataset(x_train, 4)\nx_test_3 = rotate_dataset(x_test, 4)\ntask_3 = [(x_train_3, t_train), (x_test_3, t_test)]\n\n# task 4\nx_train_4 = rotate_dataset(x_train, 3)\nx_test_4 = rotate_dataset(x_test, 3)\ntask_4 = [(x_train_4, t_train), (x_test_4, t_test)]\n\n# task 5\nx_train_5 = rotate_dataset(x_train, 9)\nx_test_5 = rotate_dataset(x_test, 9)\ntask_5 = [(x_train_5, t_train), (x_test_5, t_test)]\n\n# task 6\nx_train_6 = rotate_dataset(x_train, 5)\nx_test_6 = rotate_dataset(x_test, 5)\ntask_6 = [(x_train_6, t_train), (x_test_6, t_test)]\n\n# task 7\nx_train_7 = rotate_dataset(x_train, 8)\nx_test_7 = rotate_dataset(x_test, 8)\ntask_7 = [(x_train_7, t_train), (x_test_7, t_test)]\n\n\n# task 8\nx_train_8 = rotate_dataset(x_train, 2)\nx_test_8 = rotate_dataset(x_test, 2)\ntask_8 = [(x_train_8, t_train), (x_test_8, t_test)]\n\n# task 9\nx_train_9 = rotate_dataset(x_train, 0)\nx_test_9 = rotate_dataset(x_test, 0)\ntask_9 = [(x_train_9, t_train), (x_test_9, t_test)]\n\n# task 10\nx_train_10 = rotate_dataset(x_train, 1)\nx_test_10 = rotate_dataset(x_test, 1)\ntask_10 = [(x_train_10, t_train), (x_test_10, t_test)]\n\n\n# task list\ntasks=[task_1, task_2, task_3,task_4,task_5,task_6,task_7, task_8,task_9,task_10]\n# tasks=[task_1, task_2, task_3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # **Part III: Implement Elastic Weights Consolidation (EWC) Strategy**"},{"metadata":{"id":"uvK-RAqvcWLk"},"cell_type":"markdown","source":"* **Introduction to the method:**\n\nAccroding to paper [**Overcoming catastrophic forgetting in neural networks**](https://arxiv.org/pdf/1612.00796.pdf), EWC can prevent catastrophic forgetting by constraining important parameters to stay close to their old values when performing new tasks. \n\n\nWhile learning new task B, EWC therefore protects the performance in task A by constraining the parameters to\nstay in a region of low error for task A centered around the set of weights and biases $\\theta_A$\n\nEWC also uses a posterior probability to justify this choice of constraint and decides which weights are most important for a task. In addition, it approximates the posterior as a Gaussian distribution with mean given by the parameters $\\theta_A$ and a diagonal precision given by the diagonal of the Fisher information matrix F.\n\nHere's the loss fucnction we're going to use in our project: \n\n# $L(\\theta) = L_{B}(\\theta) + \\sum_{i}^{N}\\frac{2}{\\lambda}F_i(\\theta_i-\\theta_{i,A}^{*})^2$\n\nHere, the **$ L_{B}(\\theta) $ ** is the loss function for new task only, the ** $\\lambda$ ** is the ewc lambda, which decides how important the previous task's parameters are to the new task, we also have **$i$** labels each parameter.\n\nWhen EWC moving with the third task or more task, it will try to keep the network parameters close to the\nlearned parameters previous tasks since the sum of two quadratic penalties is itself a quadratic penalty.\n"},{"metadata":{},"cell_type":"markdown","source":"* **Inplementation of EWC:**"},{"metadata":{"id":"86-RMUNQcRAy","trusted":true},"cell_type":"code","source":"# Init dict/list for saving fisher information matrix and optmizated parameters from old task\nfisher_dict = {}\noptpar_dict = {}\n# Init ewc lamda, which sets how important the old task is compared to the new one and i labels each parameter\n# Experienment: how lamda affects the results of accuracy? This will be explained in the Part IV which will be shown later\n# From our experiment on lambda, we decided that the best EWC lambda for our model and dataset is 0.35.\newc_lambda = 0.35","execution_count":null,"outputs":[]},{"metadata":{"id":"4m5w4bibcbog","trusted":true},"cell_type":"code","source":"# Set up model and optimizer\nmodel = MLP().to(device)\n# For EWC, choese to use per-dimension learning rate method for gradient descent called ADADELTA. \n# The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent.\noptimizer = optim.Adadelta(model.parameters())\nbatch_size = 200","execution_count":null,"outputs":[]},{"metadata":{"id":"MFQfj0vVcg6X","trusted":true},"cell_type":"code","source":"def on_task_update(task_id, x_mem, t_mem):\n\n  model.train()\n  optimizer.zero_grad()\n  \n  # Satrt to accumulate gradients\n  for start in range(0, len(t_mem)-1, batch_size):\n      end = start + batch_size\n      x, y = torch.from_numpy(x_mem[start:end]), torch.from_numpy(t_mem[start:end]).long()\n      x, y = x.to(device), y.to(device)\n    \n#     Convert the input to the standard shape for MLP \n      x = x.view(x.size(0), -1)\n        \n      output = model(x)\n#     Use cross entropy for calculating loss function\n      loss = F.cross_entropy(output, y)\n#     This comes from pytorch's optimizer class, loss.backward() gives params after gradient decents   \n      loss.backward()\n\n  fisher_dict[task_id] = {}\n  optpar_dict[task_id] = {}\n\n  # use the accumulated gradients, we can now calculate the Fisher Information matrix, which is a diagnal metrix that shows covariance \n  for name, param in model.named_parameters():\n#   This is optmized parameters: \n    optpar_dict[task_id][name] = param.data.clone()\n#   This is our Fisher Matrix:\n    fisher_dict[task_id][name] = param.grad.data.clone().pow(2)\n#   print(fisher_dict)","execution_count":null,"outputs":[]},{"metadata":{"id":"cmkG6r-lcj6Q","trusted":true},"cell_type":"code","source":"def train_ewc(model, device, task_id, x_train, t_train, optimizer, epoch, e_lambda):\n    model.train()\n    for start in range(0, len(t_train)-1,batch_size):\n      end = start + batch_size\n      x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n      x, y = x.to(device), y.to(device)\n    \n      x = x.view(x.size(0), -1)\n#     Convert the input to the standard shape for MLP       \n      optimizer.zero_grad()\n\n#     output = F.log_softmax(model(x), dim=1)\n      output = model(x)\n      loss = F.cross_entropy(output, y)\n      \n#     Here we calculate our new EWC loss function which shown eailer \n      for task in range(task_id):\n        for name, param in model.named_parameters():\n          fisher = fisher_dict[task][name]\n          optpar = optpar_dict[task][name]\n#         This is Where we implement the formula of EWC loss funciton: \n#         We use current task loss  plus the loss that relevant to previous task: \n#         optpar is our optimized parameters, param is the paramters from the previous task: \n          loss += (fisher * (optpar - param).pow(2)).sum() * e_lambda\n#     Do backprop     \n      loss.backward()\n      optimizer.step()\n      #print(loss.item())\n    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))","execution_count":null,"outputs":[]},{"metadata":{"id":"DHJfHlV2cqj4","trusted":true},"cell_type":"code","source":"ewc_accs = []\n#  Run the model \nfor id, task in enumerate(tasks):\n  avg_acc = 0\n  print(\"Training on task: \", id)\n  \n  (x_train, t_train), _ = task\n\n  for epoch in range(1, 10):\n    train_ewc(model, device, id, x_train, t_train, optimizer, epoch,ewc_lambda)\n  on_task_update(id, x_train, t_train)\n    \n  for id_test, task in enumerate(tasks):\n    print(\"Testing on task: \", id_test)\n    _, (x_test, t_test) = task\n    acc = test(model, device, x_test, t_test)\n    avg_acc = avg_acc + acc\n   \n  print(\"Avg acc: \", avg_acc / 10)\n  ewc_accs.append(avg_acc / 10)","execution_count":null,"outputs":[]},{"metadata":{"id":"9fEZsmascuxv","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nplt.plot([1, 2, 3,4,5,6,7,8,9,10], ewc_accs, '-o', label=\"EWC\", color = 'g')\nplt.xlabel('Tasks number', fontsize=14)\nplt.ylabel('Average Accuracy', fontsize=14)\n\nax.grid()\n# plt.title('CL Strategies Comparison on MNSIT', fontsize=14);\nplt.xticks([1, 2, 3,4,5,6,7,8,9,10])\nplt.legend(prop={'size': 16});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we imporved the EWC model from the starter code, we can see the acc for EWC imporved from the benchmark's 74% to85.78%.**"},{"metadata":{},"cell_type":"markdown","source":"# Part IV: Explore on the ewc lambda: \nSince the **$\\lambda$** in our EWC loss function decides how important the previous task's parameters when compared to the new task, so we would want to find the best **$\\lambda$**  that fits our model which provides better accurracy: \n\nIn this part, we explores different  **$\\lambda$** values and run each of them for three tasks and then plot them together to find the best **$\\lambda$** value for our model. \n\nFrom the experiment, we can see the best **$\\lambda$** value for our MNIST dataset is 0.35, which we used in our PART III for compute the loss fucntion. "},{"metadata":{"trusted":true},"cell_type":"code","source":"lam_dic = []\nfor i in range(10,100,5):\n    test_lam = round(i*0.01,2)\n    lam_dic.append(test_lam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nlam_acc =[]\n\nfor lambda_i in lam_dic: \n    print(\"current lambda: \", lambda_i)\n    lam_diff_ewc_acc = []\n    model = MLP().to(device)\n    optimizer = optim.Adadelta(model.parameters())\n    for id, task in enumerate(tasks[:3]):\n        avg_acc = 0\n        print(\"Training on task: \", id)\n\n        (x_train, t_train), _ = task\n\n        for epoch in range(1, 3):\n            train_ewc(model, device, id, x_train, t_train, optimizer, epoch,lambda_i)\n            on_task_update(id, x_train, t_train)\n\n        for id_test, task in enumerate(tasks[:3]):\n            print(\"Testing on task: \", id_test)\n            _, (x_test, t_test) = task\n            acc = test(model, device, x_test, t_test)\n            avg_acc = avg_acc + acc\n\n        print(\"Avg acc: \", avg_acc / 3)\n        lam_diff_ewc_acc.append(avg_acc / 3)\n    lam_acc.append(lam_diff_ewc_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(50,100))\n\nn = 0.1\nfor i in lam_acc: \n    plt.plot([1, 2, 3],i, '-o', label= n)\n    plt.xlabel('Tasks number', fontsize=14)\n    plt.ylabel('Average Accuracy', fontsize=14)\n    ax.grid()\n    # plt.title('CL Strategies Comparison on MNSIT', fontsize=14);\n    plt.xticks([1, 2, 3])\n    plt.legend(prop={'size': 16})\n    n += 0.05\n    n = round(n,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part V: Compare two models\n\nOur group decided to try out two different models, one is the EWC that u can see from this notebook.\n\nAnother group member used Gradient Episodic Memory (GEM) learning strategy. Since it will be too many code in one notebook and we do have different preferance on different tools so we decided to put our two strategies in two different notebooks. \n\nGradient Episodic Memory (GEM) learning strategy code can be found at: \nhttps://colab.research.google.com/drive/1rgihSIEjvY0EdNOOLzKjcX11uT5olnCQ\n\n\n"}],"metadata":{"colab":{"name":"proj2_Lynn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}